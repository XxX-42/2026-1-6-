基于多模态情感分析的研究和应用
可行性分析报告


一、技术可行性

本课题拟采用的三塔架构（BERT + ResNet + OpenSMILE）结合早期融合（Early Fusion）的技术方案，在当前深度学习领域已具备充分的理论基础与工程实践支撑。从算法成熟度角度来看，BERT 作为自然语言处理领域的里程碑式模型，自 2018 年由 Google 发布以来，已在学术界和工业界得到了广泛验证，其中文预训练版本 bert-base-chinese 在 HuggingFace 模型库中提供了开箱即用的接口，开发者只需数行代码即可完成模型加载与推理。ResNet-50 作为计算机视觉领域的经典骨干网络，其残差连接设计有效解决了深层网络的梯度消失问题，在 ImageNet 大规模图像分类任务上的卓越表现使其成为视觉特征提取的首选方案。OpenSMILE 则是音频信号处理领域的标准工具包，支持批量提取 MFCC、Chroma 等声学特征，其稳定性和跨平台兼容性已在多个国际学术竞赛中得到验证。

从硬件适配性角度来看，本项目的核心挑战在于如何在消费级显卡（NVIDIA RTX 3060 Laptop，6GB 显存）上完成多模态模型的联合训练。针对这一约束，我们设计了一套完整的显存优化策略。冻结骨干网络（Freeze Backbone）技术允许我们在训练初期锁定 BERT 和 ResNet 的预训练参数，仅更新分类头和声学塔的权重，这一操作可将可训练参数量从约 1.5 亿骤降至约 500 万，显存占用随之大幅降低。梯度累积（Gradient Accumulation）技术则通过累积多次前向传播的梯度后再执行参数更新，在物理 Batch Size 受限的情况下等效实现大批量训练的效果，确保模型收敛的稳定性。此外，PyTorch 框架原生支持的自动混合精度训练（AMP）可将部分计算转为 FP16 半精度浮点数，在几乎不损失模型精度的前提下进一步压缩显存消耗。上述策略的组合已在多个开源项目中被验证有效，足以支撑本课题在现有硬件条件下的顺利实施。

从开发环境角度来看，PyTorch 作为当前最流行的深度学习框架之一，其动态图机制极大地降低了模型调试的难度，丰富的社区生态提供了从数据加载、模型定义到训练监控的全套工具链。HuggingFace Transformers 库简化了预训练语言模型的调用流程，torchvision 库则为计算机视觉任务提供了标准化的数据增强与模型加载接口。MTCNN 作为成熟的人脸检测算法，其 Python 实现已集成于多个开源项目中，可直接用于视频帧的人脸定位与对齐。综合来看，本项目所依赖的全部技术组件均已具备成熟的开源实现，开发过程中无需从零构建核心算法，技术可行性得到充分保障。


二、经济可行性

本课题的实施成本几乎为零，所有核心资源均可通过合法渠道免费获取。在数据层面，清华大学于 2020 年发布的 CH-SIMS 数据集是目前公开可用的高质量中文多模态情感数据集之一，其官方提供了免费的学术使用授权，研究者只需在线填写申请表即可获得下载链接。该数据集包含 2,281 个视频片段，涵盖训练、验证和测试集的标准划分，足以支撑本课题的消融实验与模型评估需求。

在模型层面，BERT 的中文预训练权重托管于 HuggingFace 模型仓库，完全开放下载且无商业使用限制。ResNet-50 的 ImageNet 预训练权重同样内置于 torchvision 库中，可通过一行代码直接加载。OpenSMILE 工具包采用开源协议发布，支持学术与非商业用途的免费使用。这意味着本项目的特征提取与模型构建环节无需支付任何软件许可或数据购买费用。

在硬件层面，本课题的全部实验将在研究者自有设备上完成。实验主机配备 AMD Ryzen 9 5900HX 处理器、64GB DDR4 内存以及 NVIDIA GeForce RTX 3060 Laptop GPU（6GB 显存），完全满足三塔模型的训练需求。由于采用了上述显存优化策略，无需租用云端高性能计算资源（如 A100 GPU 实例），避免了额外的算力支出。综合来看，本项目在经济层面不产生任何直接成本，符合本科毕业设计的资源约束条件。


三、操作可行性

从系统易用性角度来看，本项目计划在完成模型训练后构建一个基于 Gradio 的交互式演示界面。Gradio 是一款轻量级的 Python Web 应用框架，专为机器学习模型的快速部署而设计，开发者只需定义输入输出接口即可自动生成可交互的前端页面。借助该框架，用户将能够直接上传短视频片段，系统在后端完成多模态特征提取与情感预测后，实时返回分类结果与各模态的置信度分布可视化图表。这种端到端的演示方案无需用户具备任何编程基础，极大地降低了系统的使用门槛。

从代码可维护性角度来看，本项目采用模块化的工程设计原则。数据预处理管线、三塔模型定义、训练循环与评估脚本将被拆分为独立的 Python 模块，彼此通过清晰的接口进行交互。每个模块均配有详细的文档注释与单元测试用例，确保代码的可读性与可复现性。这种设计不仅便于开发过程中的迭代调试，也为后续研究者的二次开发提供了便利。综合来看，本项目在操作层面具备良好的用户友好性与工程规范性。


四、法律与社会可行性

在数据合规性方面，CH-SIMS 数据集由清华大学多模态智能实验室公开发布，面向全球学术研究者提供免费使用授权。该数据集的样本来源于公开发行的电影、电视剧及综艺节目，不涉及任何个人隐私或敏感信息的采集。研究者在申请使用时需签署学术使用协议，承诺仅将数据用于非商业性质的科研活动，这与本课题的毕业设计定位完全契合。

在伦理风险方面，本项目的研究目标是构建自动化的情感分析工具，其应用场景包括舆情监控、智能客服与人机交互等领域。系统的输出结果仅为情感类别的概率估计，不涉及对个体行为的判定或决策干预，因此不存在算法歧视或隐私侵犯等伦理隐患。此外，本项目全程在本地设备上进行，不涉及用户数据的网络传输与云端存储，进一步降低了数据泄露的风险。综合来看，本课题的实施符合学术研究的伦理规范与法律要求。


五、进度可行性

本课题的时间规划从 2026 年 1 月延续至 5 月底，共计五个月。考虑到多模态情感分析任务的复杂性，我们将项目周期划分为三个明确的阶段，每个阶段均设有具体的里程碑目标，确保整体进度可控。

第一阶段（1 月至 2 月）为数据预处理与环境搭建期。该阶段的核心任务是跑通完整的数据处理管线，包括利用 MTCNN 完成视频帧的人脸检测与对齐、利用 OpenSMILE 批量提取声学特征、以及解析 CH-SIMS 官方提供的词级时序对齐文件。两个月的时间窗口为该阶段预留了充足的调试缓冲，确保后续模型训练建立在可靠的数据基础之上。

第二阶段（3 月）为模型构建与基线验证期。该阶段将依次实现文本塔、视觉塔与声学塔，并验证各模块的独立运行正确性。在此基础上，三塔融合模型将被组装并进行初步训练，显存优化策略（冻结骨干、梯度累积、混合精度）将在此阶段逐一调试定型。该阶段结束时，预期实现三塔模型在验证集上的 Loss 稳定下降。

第三阶段（4 月至 5 月）为实验验证与论文撰写期。该阶段将系统开展七组消融实验，量化各模态及融合策略的贡献，并绘制性能对比图表。实验数据收齐后，将按照学校规范撰写毕业设计论文，同步完成答辩 PPT 与 Gradio 演示界面的制作。五月下旬为论文修订与终稿定稿预留了充足时间。

综合来看，上述三阶段规划遵循"先数据、后模型、再实验"的工程逻辑，各阶段任务量分配均衡，时间节点设置合理。只要严格按照里程碑推进，本课题完全能够在规定时间内高质量完成。
